{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1870698,"sourceType":"datasetVersion","datasetId":1103863}],"dockerImageVersionId":30527,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow.keras.utils as ku\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import regularizers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reading the text data file\ndata = open('/kaggle/input/poem-generation/poem.txt', encoding=\"utf8\").read()\n \n# Plotting the WordCloud\ndata[:100]","metadata":{"execution":{"iopub.status.busy":"2023-07-29T12:28:16.522444Z","iopub.execute_input":"2023-07-29T12:28:16.524526Z","iopub.status.idle":"2023-07-29T12:28:16.540711Z","shell.execute_reply.started":"2023-07-29T12:28:16.52449Z","shell.execute_reply":"2023-07-29T12:28:16.539811Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus = data.lower().split(\"\\n\")\ncorpus[:15]","metadata":{"execution":{"iopub.status.busy":"2023-07-29T12:29:03.466795Z","iopub.execute_input":"2023-07-29T12:29:03.467186Z","iopub.status.idle":"2023-07-29T12:29:03.478395Z","shell.execute_reply.started":"2023-07-29T12:29:03.46713Z","shell.execute_reply":"2023-07-29T12:29:03.477221Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(corpus)","metadata":{"execution":{"iopub.status.busy":"2023-07-29T12:29:33.85815Z","iopub.execute_input":"2023-07-29T12:29:33.858537Z","iopub.status.idle":"2023-07-29T12:29:33.86809Z","shell.execute_reply.started":"2023-07-29T12:29:33.858508Z","shell.execute_reply":"2023-07-29T12:29:33.867057Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fitting the Tokenizer on the Corpus\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(corpus)\n \n# Vocabulary count of the corpus\ntotal_words = len(tokenizer.word_index)\n \nprint(\"Total Words:\", total_words)","metadata":{"execution":{"iopub.status.busy":"2023-07-29T12:30:30.420274Z","iopub.execute_input":"2023-07-29T12:30:30.420634Z","iopub.status.idle":"2023-07-29T12:30:30.472867Z","shell.execute_reply.started":"2023-07-29T12:30:30.420605Z","shell.execute_reply":"2023-07-29T12:30:30.471503Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.word_index['i']","metadata":{"execution":{"iopub.status.busy":"2023-07-29T12:33:12.906256Z","iopub.execute_input":"2023-07-29T12:33:12.906611Z","iopub.status.idle":"2023-07-29T12:33:12.913801Z","shell.execute_reply.started":"2023-07-29T12:33:12.90658Z","shell.execute_reply":"2023-07-29T12:33:12.912615Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Converting the text into embeddings\ninput_sequences = []\nfor line in corpus:\n    token_list = tokenizer.texts_to_sequences([line])[0]\n \n    for i in range(1, len(token_list)):\n        n_gram_sequence = token_list[:i+1]\n        input_sequences.append(n_gram_sequence)\n \nmax_sequence_len = max([len(x) for x in input_sequences])\ninput_sequences = np.array(pad_sequences(input_sequences,\n                                         maxlen=max_sequence_len,\n                                         padding='pre'))\npredictors, label = input_sequences[:, :-1], input_sequences[:, -1]\nlabel = ku.to_categorical(label, num_classes=total_words+1)","metadata":{"execution":{"iopub.status.busy":"2023-07-29T12:32:30.307873Z","iopub.execute_input":"2023-07-29T12:32:30.308275Z","iopub.status.idle":"2023-07-29T12:32:30.470103Z","shell.execute_reply.started":"2023-07-29T12:32:30.308242Z","shell.execute_reply":"2023-07-29T12:32:30.46906Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_sequences","metadata":{"execution":{"iopub.status.busy":"2023-07-29T12:33:54.206595Z","iopub.execute_input":"2023-07-29T12:33:54.206938Z","iopub.status.idle":"2023-07-29T12:33:54.214081Z","shell.execute_reply.started":"2023-07-29T12:33:54.20691Z","shell.execute_reply":"2023-07-29T12:33:54.212993Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Building a Bi-Directional LSTM Model\nmodel = Sequential()\nmodel.add(Embedding(total_words+1, 100,\n                    input_length=max_sequence_len-1))\nmodel.add(Bidirectional(LSTM(150, return_sequences=True)))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(100))\nmodel.add(Dense(total_words+1/2, activation='relu',\n                kernel_regularizer=regularizers.l2(0.01)))\nmodel.add(Dense(total_words+1, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam', metrics=['accuracy'])\nprint(model.summary())","metadata":{"execution":{"iopub.status.busy":"2023-07-29T12:36:55.779184Z","iopub.execute_input":"2023-07-29T12:36:55.780178Z","iopub.status.idle":"2023-07-29T12:37:01.600414Z","shell.execute_reply.started":"2023-07-29T12:36:55.780119Z","shell.execute_reply":"2023-07-29T12:37:01.599568Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(predictors, label, epochs=150, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2023-07-29T12:38:12.385485Z","iopub.execute_input":"2023-07-29T12:38:12.38584Z","iopub.status.idle":"2023-07-29T12:54:39.504365Z","shell.execute_reply.started":"2023-07-29T12:38:12.385811Z","shell.execute_reply":"2023-07-29T12:54:39.503307Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_text = \"The world\"\nnext_words = 25\nouptut_text = \"\"\n \nfor _ in range(next_words):\n    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n    token_list = pad_sequences(\n        [token_list], maxlen=max_sequence_len-1,\n      padding='pre')\n    predicted = np.argmax(model.predict(token_list,\n                                        verbose=0), axis=-1)\n    output_word = \"\"\n     \n    for word, index in tokenizer.word_index.items():\n        if index == predicted:\n            output_word = word\n            break\n             \n    seed_text += \" \" + output_word\n     \nprint(seed_text)","metadata":{"execution":{"iopub.status.busy":"2023-07-29T13:00:56.255513Z","iopub.execute_input":"2023-07-29T13:00:56.25589Z","iopub.status.idle":"2023-07-29T13:00:58.801917Z","shell.execute_reply.started":"2023-07-29T13:00:56.255858Z","shell.execute_reply":"2023-07-29T13:00:58.800852Z"},"editable":false,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"editable":false},"execution_count":null,"outputs":[]}]}